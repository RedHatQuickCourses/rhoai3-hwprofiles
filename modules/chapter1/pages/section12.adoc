= Integrating Hardware Profiles with Kueue
:author: Platform Engineering Team
:description: Enabling project-level quota management using Kueue in RHOAI.
:toc: left

== Overview
Kueue provides a two-layer queuing system:
1. **LocalQueue (Namespace Scope):** The entry point for user workloads.
2. **ClusterQueue (Cluster Scope):** The pool of resources where quotas are enforced.

To make our tiered `HardwareProfiles` work with Kueue, we must "activate" each project and map the profiles to corresponding **ResourceFlavors**.

== Step 1: Activate Kueue on a Project
OpenShift AI uses a label-based "opt-in" system for Kueue. Run the following command to enable the Kueue webhook for your specific data science project.

[source,bash]
----
# Replace <project-name> with your target namespace
oc label namespace <project-name> kueue.openshift.io/managed=true
----

[NOTE]
Once labeled, the Kueue operator will intercept pod creation in this namespace and ensure they are assigned to a queue.

== Step 2: Define ResourceFlavors
A **ResourceFlavor** represents the physical reality of your hardware. We will create three flavors to match our strategy: `default`, `shared-gpu`, and `dedicated-a100`.

[source,yaml]
----
# Standard Worker Nodes
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: default-flavor
---
# Shared GPU Nodes (Time-Slicing)
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: shared-gpu-flavor
spec:
  nodeLabels:
    nvidia.com/device-plugin.config: "time-slicing-config"
---
# Dedicated A100 Training Nodes
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: a100-flavor
spec:
  nodeLabels:
    nvidia.com/gpu.product: "A100-SXM4-80GB"
  tolerations:
    - key: "workload"
      operator: "Equal"
      value: "training"
      effect: "NoSchedule"
----

== Step 3: Configure the ClusterQueue
The **ClusterQueue** is where you set the actual hardware limits (quotas) for the cluster.

[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: main-cluster-queue
spec:
  namespaceSelector: {} # Allows all managed namespaces to use this queue
  resourceGroups:
    - coveredResources: ["cpu", "memory", "nvidia.com/gpu"]
      flavors:
        - name: default-flavor
          resources:
            - name: "cpu"
              nominalQuota: "100"
            - name: "memory"
              nominalQuota: "500Gi"
        - name: shared-gpu-flavor
          resources:
            - name: "nvidia.com/gpu"
              nominalQuota: "8"
        - name: a100-flavor
          resources:
            - name: "nvidia.com/gpu"
              nominalQuota: "4"
----

== Step 4: Create the LocalQueue
Each project needs a **LocalQueue** that points to the `main-cluster-queue`. We also add an annotation to make it the "Default" queue for that project.

[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: default-queue
  namespace: <your-project-name>
  annotations:
    kueue.x-k8s.io/default-queue: "true"
spec:
  clusterQueue: main-cluster-queue
----

== Step 5: Mapping Hardware Profiles to Kueue
When a user selects a `HardwareProfile` in the RHOAI Dashboard, the operator creates a pod with specific resource requests. Kueue automatically maps these to the correct flavor:

* **CPU-only Profiles:** Map to `default-flavor`.
* **Shared GPU Profiles:** Map to `shared-gpu-flavor` because of the `nodeSelector` in the profile.
* **A100 Profiles:** Map to `a100-flavor` because of the `nodeSelector` and `tolerations` in the profile.



= Mapping Hardware Profiles to Kueue Quotas
:author: Platform Engineering Team
:description: Connecting RHOAI HardwareProfiles to Kueue for automated resource allocation.
:toc: left

== Overview
To ensure a `HardwareProfile` actually consumes quota from a specific Kueue `ClusterQueue`, we must ensure the Pods generated by the profile match the labels and tolerations of our **ResourceFlavors**.

This is achieved by aligning the `nodeSelector` and `tolerations` in the profile with the `nodeLabels` and `tolerations` in the ResourceFlavor.

== Step 1: Connecting Profiles to LocalQueues
In the `v1` API, you can specify which `LocalQueue` a profile should use by default. This ensures that users don't have to manually select a queue when starting a workbench.

[source,yaml]
----
# Example: Adding a default queue to a profile
metadata:
  annotations:
    opendatahub.io/dashboard-feature-visibility: '["workbench"]'
spec:
  # This tells the RHOAI dashboard which LocalQueue to associate with the workbench
  # Note: Requires the namespace to have a LocalQueue with this name
  localQueue: "default-queue" 
----

== Step 2: Example Profiles for Kueue Integration

=== Example A: General Purpose (Default Flavor)
This profile has no specific selectors, so it automatically falls into the `default-flavor` in your `ClusterQueue`.

[source,yaml]
----
apiVersion: infrastructure.opendatahub.io/v1
kind: HardwareProfile
metadata:
  name: k-standard-cpu
  namespace: redhat-ods-applications
  annotations:
    opendatahub.io/display-name: "K-Standard: CPU"
    opendatahub.io/description: "Managed by Kueue (Default Quota)"
spec:
  enabled: true
  identifiers:
    - { identifier: cpu, displayName: CPU, resourceType: CPU, defaultCount: 2, minCount: 1, maxCount: 4 }
    - { identifier: memory, displayName: Memory, resourceType: Memory, defaultCount: 4Gi, minCount: 4Gi, maxCount: 8Gi }
----

=== Example B: Shared GPU (Shared Flavor)
This profile includes a `nodeSelector` that matches our `shared-gpu-flavor`. Kueue will only admit this if there is "Shared GPU" quota available.

[source,yaml]
----
apiVersion: infrastructure.opendatahub.io/v1
kind: HardwareProfile
metadata:
  name: k-gpu-shared
  namespace: redhat-ods-applications
  annotations:
    opendatahub.io/display-name: "K-GPU: Shared V100"
spec:
  enabled: true
  identifiers:
    - { identifier: "nvidia.com/gpu", displayName: "GPU", resourceType: Accelerator, defaultCount: 1, minCount: 1, maxCount: 1 }
  # The "Trigger" for Kueue's shared-gpu-flavor
  nodeSelector:
    nvidia.com/device-plugin.config: "time-slicing-config"
----

=== Example C: High-Priority Training (A100 Flavor)
This profile uses taints and node selectors to trigger the `a100-flavor` quota and ensure placement on high-end hardware.

[source,yaml]
----
apiVersion: infrastructure.opendatahub.io/v1
kind: HardwareProfile
metadata:
  name: k-training-a100
  namespace: redhat-ods-applications
  annotations:
    opendatahub.io/display-name: "K-Training: A100 Dedicated"
spec:
  enabled: true
  identifiers:
    - { identifier: "nvidia.com/gpu", displayName: "A100", resourceType: Accelerator, defaultCount: 1, minCount: 1, maxCount: 8 }
  nodeSelector:
    nvidia.com/gpu.product: "A100-SXM4-80GB"
  tolerations:
    - key: "workload"
      operator: "Equal"
      value: "training"
      effect: "NoSchedule"
----

== Step 3: Verification Commands
Once a user starts a workbench using one of these profiles, use these commands to see Kueue in action:

.Check if the workload was admitted:
[source,bash]
----
oc get workloads -n <user-project>
----

.Check which flavor was assigned:
[source,bash]
----
oc get workload <workload-name> -n <user-project> -o yaml | grep flavor
----

.See current quota consumption:
[source,bash]
----
oc describe clusterqueue main-cluster-queue
----


= Integrating Hardware Profiles with Kueue
:author: Platform Engineering Team
:description: Enabling project-level quota management using Kueue in RHOAI.
:toc: left

== Overview
Kueue provides a two-layer queuing system:
1. **LocalQueue (Namespace Scope):** The entry point for user workloads.
2. **ClusterQueue (Cluster Scope):** The pool of resources where quotas are enforced.

== Step 1: Activate Kueue on a Project
Run the following command to enable the Kueue webhook for your specific data science project.

[source,bash]
----
# Replace <project-name> with your target namespace
oc label namespace <project-name> kueue.openshift.io/managed=true
----

== Step 2: Define ResourceFlavors
A **ResourceFlavor** defines the taints and labels of your physical nodes.

[source,yaml]
----
# Standard Worker Nodes
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: default-flavor
---
# Dedicated A100 Training Nodes
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: a100-flavor
spec:
  nodeLabels:
    nvidia.com/gpu.product: "A100-SXM4-80GB"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
----

== Step 3: Deployable Hardware Profile Examples
These profiles are configured for the `redhat-ods-applications` namespace. Instead of manual node selectors, they use the `localQueue` field to point to Kueue.

=== 1. Standard CPU Profile (Auto-Injected)
This profile utilizes the `default-flavor`. It doesn't need selectors; Kueue will place it on any available worker.

[source,yaml]
----
apiVersion: infrastructure.opendatahub.io/v1
kind: HardwareProfile
metadata:
  name: k-standard-cpu
  namespace: redhat-ods-applications
  annotations:
    opendatahub.io/display-name: "K-Standard: CPU"
    opendatahub.io/dashboard-feature-visibility: '["workbench"]'
spec:
  enabled: true
  # Logic: By selecting 'default-queue', Kueue manages the placement
  managementState: "Managed"
  location:
    localQueue: "default-queue"
  identifiers:
    - identifier: cpu
      displayName: CPU
      resourceType: CPU
      defaultCount: 2
      minCount: 1
      maxCount: 4
    - identifier: memory
      displayName: Memory
      resourceType: Memory
      defaultCount: 4Gi
      minCount: 4Gi
      maxCount: 8Gi
----

=== 2. High-Priority A100 Training
This profile is "Kueue-aware." When a user selects this, Kueue matches the resource request to the `a100-flavor` and **automatically injects** the necessary tolerations and selectors into the Pod.

[source,yaml]
----
apiVersion: infrastructure.opendatahub.io/v1
kind: HardwareProfile
metadata:
  name: k-training-a100
  namespace: redhat-ods-applications
  annotations:
    opendatahub.io/display-name: "K-Training: A100 Dedicated"
    opendatahub.io/description: "Managed by Kueue: Automatic placement on A100 nodes."
spec:
  enabled: true
  managementState: "Managed"
  location:
    localQueue: "default-queue"
  identifiers:
    - identifier: "nvidia.com/gpu"
      displayName: "A100 GPU"
      resourceType: Accelerator
      defaultCount: 1
      minCount: 1
      maxCount: 8
----

== Summary: The Rules of Kueue & Profiles
1. **Do not use `nodeSelector` or `tolerations`** inside the `HardwareProfile` spec if the profile is `Managed` by Kueue.
2. **Define the labels in the `ResourceFlavor`** instead.
3. **Kueue matches the Resource Request** (e.g., `nvidia.com/gpu`) to the Flavor and applies the labels/tolerations to the Pod for you.