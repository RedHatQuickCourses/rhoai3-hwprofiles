= Lab: Optimizing AI Clusters with Kueue & Priority
:toc: left
:icons: font
:source-highlighter: rouge

== 1. The Business Value of Kueue
In high-performance AI environments, hardware like GPUs and high-core-count CPUs are your most expensive assets. Without proper orchestration:
 * **Waste occurs:** Standard low-priority pods (like web servers) might "squat" on expensive AI nodes.
 * **Low ROI:** Expensive hardware sits idle because one user "hoarded" resources they aren't currently using.
 * **Bottlenecks:** Critical production training jobs get stuck behind experimental notebooks.

**Kueue** maximizes your Return on Investment (ROI) by ensuring that resources are always doing the most valuable work possible through **Quota Management** and **Priority Preemption**.

---

== 2. Lab Environment Setup
This lab assumes the **Red Hat Build of Kueue** operator is installed and OpenShift AI 3.0 is running in `Unmanaged` mode for the Kueue component.

=== Step 2.1: Create the Project Namespace
We will create a specific namespace for our hardware profile testing.

[source,bash]
----
# Create the new project
oc new-project deploy-hwprofiles

# Label the namespace so Kueue's admission controller watches it
oc label namespace deploy-hwprofiles kueue.x-k8s.io/ignore-separator=true --overwrite
----

---

== 3. Exercise 1: Building the Basic CPU Queue
In this exercise, we define a "Hard Limit" for our project. This prevents "Resource Sprawl" where one project consumes the entire cluster's CPU.

=== Step 3.1: Create Resource Flavor and ClusterQueue
Run the following to define a global pool of **4 CPUs**.

[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: default-flavor
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: cluster-queue
spec:
  namespaceSelector: {} 
  resourceGroups:
  - coveredResources: ["cpu", "memory"]
    flavors:
    - name: default-flavor
      resources:
      - name: "cpu"
        nominalQuota: 4
      - name: "memory"
        nominalQuota: 16Gi
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: user-queue
  namespace: deploy-hwprofiles
spec:
  clusterQueue: cluster-queue
EOF
----

---

== 4. Exercise 2: Testing Priority and Preemption
Now we will simulate a real-world conflict: A "Standard" user takes up the resources, and a "VIP" user needs to jump the line.

=== Step 4.1: Define Priority Tiers
[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: kueue.x-k8s.io/v1beta1
kind: WorkloadPriorityClass
metadata:
  name: vip-priority
value: 1000
description: "High priority for production AI jobs"
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: WorkloadPriorityClass
metadata:
  name: standard-priority
value: 100
description: "Standard priority for development jobs"
EOF
----

=== Step 4.2: Enable Preemption on the Queue
[source,bash]
----
oc patch clusterqueue cluster-queue --type merge -p '{"spec": {"preemption": {"withinClusterQueue": "LowerPriority"}}}'
----

=== Step 4.3: Start a "Standard" Job (The Resource Occupier)
This job asks for **3 CPUs**, leaving only 1 available in our 4-CPU quota.

[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: standard-dev-job
  namespace: deploy-hwprofiles
  labels:
    kueue.x-k8s.io/queue-name: user-queue
    kueue.x-k8s.io/priority-class: standard-priority
spec:
  template:
    spec:
      containers:
      - name: dummy
        image: busybox
        command: ["sleep", "600"]
        resources:
          requests:
            cpu: "3"
      restartPolicy: Never
EOF
----

=== Step 4.4: Start a "VIP" Job (The High ROI Task)
This job requires **2 CPUs**. Since 3 are taken, it normally would wait. Because of **Preemption**, it will evict the standard job.

[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: production-vip-job
  namespace: deploy-hwprofiles
  labels:
    kueue.x-k8s.io/queue-name: user-queue
    kueue.x-k8s.io/priority-class: vip-priority
spec:
  template:
    spec:
      containers:
      - name: dummy
        image: busybox
        command: ["sleep", "600"]
        resources:
          requests:
            cpu: "2"
      restartPolicy: Never
EOF
----

---

== 5. Observing the Result
Navigate to the **OpenShift AI Dashboard** -> **Distributed Workloads**.

[cols="1,3"]
|===
| **Observation** | **Explanation**

| **Job Eviction** | Use `oc get pods`. You will see `standard-dev-job` pods terminating to make room.

| **Dashboard Status** | The VIP job will show as "Running" and the Standard job will show as "Pending" or "Evicted".

| **ROI Success**
| You have successfully ensured that the most critical business task has access to compute without needing to manually scale the cluster.
|===


== 6. Exercise 3: Fair Sharing and Cohorts
In this exercise, we will create two separate projects (Team A and Team B). They will share a common pool of resources (a **Cohort**), but we will weight them so Team A is considered "more important."

=== Step 6.1: Setup the Second Project
[source,bash]
----
# Create Team B's project
oc new-project team-b-research
oc label namespace team-b-research kueue.x-k8s.io/ignore-separator=true --overwrite
----

=== Step 6.2: Define Shared Cohort and Weighted Queues
We will update the `ClusterQueues` to join the same `cohort`. We will also add `fairSharing` weights. 
 * **Team A (deploy-hwprofiles):** Weight 3 (Gets 75% of shared space)
 * **Team B (team-b-research):** Weight 1 (Gets 25% of shared space)

[source,bash]
----
cat <<EOF | oc apply -f -
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: team-a-cq
spec:
  namespaceSelector: {}
  cohort: "ai-compute-pool" # This link enables sharing
  fairSharing:
    weight: "3"
  resourceGroups:
  - coveredResources: ["cpu", "memory"]
    flavors:
    - name: default-flavor
      resources:
      - name: "cpu"
        nominalQuota: 2 # Guaranteed amount
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: team-b-cq
spec:
  namespaceSelector: {}
  cohort: "ai-compute-pool"
  fairSharing:
    weight: "1"
  resourceGroups:
  - coveredResources: ["cpu", "memory"]
    flavors:
    - name: default-flavor
      resources:
      - name: "cpu"
        nominalQuota: 2 # Guaranteed amount
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: team-b-queue
  namespace: team-b-research
spec:
  clusterQueue: team-b-cq
EOF
----

=== Step 6.3: Observe "Borrowing" in Action
If `team-b-research` submits a job requiring **4 CPUs**, it will use its own 2 CPUs and "borrow" the 2 unused CPUs from Team A. 

[source,bash]
----
# Run this to see the current 'Share' status
oc get clusterqueue -o custom-columns=NAME:.metadata.name,COHORT:.spec.cohort,WEIGHT:.spec.fairSharing.weight
----

---

== 7. Lab Conclusion: Maximizing ROI
By completing these three exercises, you have moved from a static cluster to an **Elastic AI Infrastructure**:

1.  **Stop Waste:** Through Cohorts, resources never sit idle if someone has work to do.
2.  **Ensure Predictability:** Through Fair Sharing, you have a mathematical guarantee that "Borrowers" won't block "Owners."
3.  **Protect Production:** Through Priority, you ensure that even in a full cluster, the most valuable business logic (VIP) always runs first.